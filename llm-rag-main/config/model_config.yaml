# Model Configuration
model:
  base_model: "meta-llama/Llama-3.2-1B-Instruct" # Llama 1B model
  model_name: "legal-llama-1b-lora"
  max_length: 2048
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  do_sample: true

# LoRA Configuration (optimized for 1B model)
lora:
  r: 8 # Reduced rank for smaller model
  lora_alpha: 16 # Reduced alpha
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    [
      "q_proj",
      "v_proj",
      "k_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

# Quantization (for memory efficiency) - Disabled for macOS compatibility
quantization:
  load_in_4bit: false # Disabled for macOS
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# Paths
paths:
  base_model_path: "./models/base/"
  fine_tuned_model_path: "./models/fine_tuned/"
  tokenizer_path: "./models/tokenizer/"

# Device settings
device:
  use_cuda: true
  mixed_precision: "fp16"
