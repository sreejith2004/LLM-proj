# Training Configuration (optimized for GPU systems)
training:
  output_dir: "./results"
  num_train_epochs: 3
  per_device_train_batch_size: 8  # Higher for GPU
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2  # Lower since we can use larger batches
  learning_rate: 0.0003  # Slightly higher for smaller model (3e-4)
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "rouge_l"
  greater_is_better: true
  report_to: []  # Disabled wandb for simplicity

# Data Configuration
data:
  train_file: "./data/processed/train.json"
  validation_file: "./data/processed/validation.json"
  test_file: "./data/processed/test.json"
  max_source_length: 1024  # Full length for GPU
  max_target_length: 256
  preprocessing_num_workers: 4

# Early Stopping
early_stopping:
  patience: 3
  threshold: 0.001

# Optimizer
optimizer:
  name: "adamw_torch"
  lr_scheduler_type: "cosine"

# Logging
logging:
  project_name: "legal-llm-finetune"
  run_name: "legal-summarization-llama1b-gpu-v1"
